{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Question_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'Data\\training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(path, encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Header row to data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "path = r'Data\\training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(path, names=columns, header=None, encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5000 Sample from each polarity(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1974671194</td>\n",
       "      <td>Sat May 30 13:36:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>simba98</td>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1997882236</td>\n",
       "      <td>Mon Jun 01 17:37:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Seve76</td>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2177756662</td>\n",
       "      <td>Mon Jun 15 06:39:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>x__claireyy__x</td>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2216838047</td>\n",
       "      <td>Wed Jun 17 20:02:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Balasi</td>\n",
       "      <td>I am craving for japanese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1880666283</td>\n",
       "      <td>Fri May 22 02:03:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djrickdawson</td>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag            user  \\\n",
       "0       0  1974671194  Sat May 30 13:36:31 PDT 2009  NO_QUERY         simba98   \n",
       "1       0  1997882236  Mon Jun 01 17:37:11 PDT 2009  NO_QUERY          Seve76   \n",
       "2       0  2177756662  Mon Jun 15 06:39:05 PDT 2009  NO_QUERY  x__claireyy__x   \n",
       "3       0  2216838047  Wed Jun 17 20:02:12 PDT 2009  NO_QUERY          Balasi   \n",
       "4       0  1880666283  Fri May 22 02:03:31 PDT 2009  NO_QUERY    djrickdawson   \n",
       "\n",
       "                                                text  \n",
       "0  @xnausikaax oh no! where did u order from? tha...  \n",
       "1  A great hard training weekend is over.  a coup...  \n",
       "2  Right, off to work  Only 5 hours to go until I...  \n",
       "3                    I am craving for japanese food   \n",
       "4  Jean Michel Jarre concert tomorrow  gotta work...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df = df.groupby('target').apply(lambda x: x.sample(5000, random_state=42))\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(sampled_df[sampled_df['target'] == 0]))\n",
    "print(len(sampled_df[sampled_df['target'] == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries for normalizing and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# Download this packages if neccessary!\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dictionary of emoticons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', \"won't\", 'their', 'herself', 'd', 'these', 'the', 'how', 'our', 'been', 'under', 'will', 'before', \"it's\", \"couldn't\", 't', 'i', 'that', 'its', 'do', 'shan', 'own', 'haven', 'those', 'my', \"don't\", 'ain', \"doesn't\", 'won', \"weren't\", 'she', 'if', 'hasn', 'are', 'his', 'other', 've', 'while', 'all', \"mustn't\", 'because', 'very', 'should', 'any', 'nor', 'about', 'yours', \"you've\", 'after', 'don', 'ma', 'a', 'themselves', 'into', 'again', 'more', 'so', \"you're\", 'where', 'myself', 'once', 'few', 'll', \"wouldn't\", \"aren't\", 'an', 'it', 'off', 'each', 'not', 'just', \"shouldn't\", 'doesn', 'during', 'but', 'he', 'down', \"hadn't\", 'what', 're', 'some', 'hadn', 'having', \"that'll\", 'theirs', 'this', 'then', 'until', 'does', 'now', 'yourselves', 'them', 'at', 'above', 'to', 'no', 'mightn', 'needn', 'isn', 'and', 'o', 'hers', 'same', 'you', 'or', 'ours', \"she's\", 'for', \"hasn't\", 'when', \"didn't\", 'against', 'as', 'wouldn', 'be', \"shan't\", \"isn't\", \"haven't\", 'did', 'has', 'himself', 'your', 'shouldn', 'yourself', 'through', 'him', 'too', \"you'll\", 'had', 'we', 'from', 'further', \"wasn't\", 'why', 'both', 'were', 'wasn', \"you'd\", 'who', 'is', 'up', 'here', 'over', 'of', 'didn', 'below', 'with', 'between', \"mightn't\", \"should've\", 'y', 's', 'couldn', 'which', 'they', 'm', 'mustn', 'weren', 'am', 'ourselves', 'her', 'was', 'me', 'have', 'in', 'whom', 'being', 'most', 'only', 'itself', 'aren', 'than', 'can', 'on', \"needn't\", 'out', 'by', 'such', 'there'}\n"
     ]
    }
   ],
   "source": [
    "emoticons_dict = {\n",
    "    \":)\": \"happy\",\n",
    "    \":-)\": \"happy\",\n",
    "    \":]\": \"happy\",\n",
    "    \":D\": \"laughing\",\n",
    "    \":-D\": \"laughing\",\n",
    "    \"XD\": \"laughing\",\n",
    "    \"xD\": \"laughing\",\n",
    "    \":(\": \"sad\",\n",
    "    \":-(\": \"sad\",\n",
    "    \":[\": \"sad\",\n",
    "    \":'(\": \"crying\",\n",
    "    \":'-)\": \"tears_of_joy\",\n",
    "    \":'D\": \"tears_of_joy\",\n",
    "    \";)\": \"wink\",\n",
    "    \";-)\": \"wink\",\n",
    "    \";D\": \"wink\",\n",
    "    \">_<\": \"frustrated\",\n",
    "    \">.<\": \"frustrated\",\n",
    "    \":-/\": \"skeptical\",\n",
    "    \":/\": \"skeptical\",\n",
    "    \":\\\\\": \"skeptical\",\n",
    "    \":-|\": \"neutral\",\n",
    "    \":|\": \"neutral\",\n",
    "    \":-P\": \"playful\",\n",
    "    \":P\": \"playful\",\n",
    "    \":O\": \"surprised\",\n",
    "    \":-O\": \"surprised\",\n",
    "    \":-0\": \"shocked\",\n",
    "    \">:(\": \"angry\",\n",
    "    \"D:<\": \"angry\",\n",
    "    \"D-':\": \"angry\",\n",
    "    \":3\": \"cute\",\n",
    "    \"O:)\": \"angelic\",\n",
    "    \"3:)\": \"devilish\",\n",
    "    \"<3\": \"love\",\n",
    "    \"</3\": \"broken_heart\",\n",
    "    \":*\": \"kiss\",\n",
    "    \":-*\": \"kiss\",\n",
    "    \";*\": \"kiss\",\n",
    "    \":-S\": \"worried\",\n",
    "    \"D:\": \"dismay\",\n",
    "    \":$\": \"embarrassed\",\n",
    "    \":-X\": \"sealed_lips\",\n",
    "    \"8-)\": \"cool\",\n",
    "    \"B-)\": \"cool\",\n",
    "    \"-_-\": \"annoyed\",\n",
    "    \"-__-\": \"annoyed\",\n",
    "    \"-___-\": \"exhausted\",\n",
    "    \":^)\": \"nose_laugh\",\n",
    "    \"|-O\": \"bored\",\n",
    "    \":@\": \"furious\",\n",
    "    \">:O\": \"upset\",\n",
    "    \":-)\": \"smiley\",\n",
    "    \"XD\": \"big_grin\",\n",
    "    \"^^\": \"joy\",\n",
    "    \"^_^\": \"joy\",\n",
    "    \":-(\": \"frowny\",\n",
    "    \"T_T\": \"cry\",\n",
    "    \"Q_Q\": \"cry\",\n",
    "    \"TT_TT\": \"cry\",\n",
    "    \">.<\": \"irritated\",\n",
    "    \"-_-;\": \"sweat\",\n",
    "    \"':(\": \"tear\",\n",
    "    \"D;\": \"horrified\",\n",
    "    \"o_O\": \"surprise\",\n",
    "    \"O_o\": \"dismay\",\n",
    "    \"o.o\": \"bewildered\",\n",
    "    \"o_o\": \"stunned\",\n",
    "    \"0:3\": \"innocent\",\n",
    "    \"O:-)\": \"angel\",\n",
    "    \"(>_<)\": \"trouble\",\n",
    "    \"(^_^;)\": \"nervous\",\n",
    "    \"(-_-;)\": \"worried\",\n",
    "    \"(T_T)\": \"upset\",\n",
    "    \"(-_-)zzz\": \"sleeping\",\n",
    "    \"(^_^)/\": \"cheer\",\n",
    "    \"(^-^)b\": \"thumbs_up\",\n",
    "    \"(>_<)>\": \"frustrated\",\n",
    "    \"(^o^)\": \"elated\",\n",
    "    \"(-_-)\": \"annoyed\",\n",
    "}\n",
    "\n",
    "acronyms_dict = {\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"tbt\": \"throwback thursday\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"stfu\": \"shut the f*** up\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"wtf\": \"what the f***\",\n",
    "    \"ftl\": \"for the loss\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "}\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, emoticons_dict, stop_words, lemmatizer, acronyms_dict):\n",
    "        self.text = None\n",
    "        self.map = emoticons_dict\n",
    "        self.stop_words_set = stop_words\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.accronyms = acronyms_dict\n",
    "    \n",
    "    def assign_text(self, text):\n",
    "        self.text = text    \n",
    "\n",
    "    def normalize_tokenize(self):\n",
    "        self.text = self._lower_case()\n",
    "        self.text = self._remove_user_mention()\n",
    "        self.text = self._remove_URLs()\n",
    "        self.text = self._remove_emalis()\n",
    "        self.text = self._replace_emoticons()\n",
    "        self.text = self._replace_emojies()\n",
    "        self.text = self._remove_URLs()\n",
    "        self.text = self._handling_hashtags()\n",
    "        self.text = self._repeated_characters() # Feel free to rerun the code based on ignoring repeated charcters or not!\n",
    "        self.text = self._remove_punctuations()\n",
    "        self.text = self._remove_non_ascii_chars()\n",
    "        self.text = self._remove_numbers()\n",
    "        self.text = self._expand_accronyms()\n",
    "        tokens = self._tokenizing()\n",
    "        tokens = self._lemmatize_tokens(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def _lower_case(self):\n",
    "        return self.text.lower()\n",
    "    \n",
    "    def _remove_user_mention(self):\n",
    "        return re.sub(r'@\\w+', '', self.text)\n",
    "    \n",
    "    def _remove_URLs(self):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', self.text, flags=re.MULTILINE)\n",
    "    \n",
    "    def _replace_emoticons(self):\n",
    "        return \" \".join(self.map.get(word, word) for word in self.text.split())\n",
    "    \n",
    "    def _replace_emojies(self):\n",
    "        return emoji.demojize(self.text, delimiters=(\"\",\"\"))\n",
    "\n",
    "    def _handling_hashtags(self):\n",
    "        return self.text.replace('#', '')\n",
    "    \n",
    "    def _repeated_characters(self):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', self.text)\n",
    "\n",
    "    def _remove_punctuations(self): # Except important one like !\n",
    "        return self.text.translate(str.maketrans('', '', string.punctuation.replace('!', '')))\n",
    "\n",
    "    def _remove_numbers(self):\n",
    "        return re.sub(r'\\b\\d+(?:,\\d+)*(?:\\.\\d+)?\\b', '', self.text)\n",
    "\n",
    "    def _remove_non_ascii_chars(self):\n",
    "        return self.text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    def _expand_accronyms(self):\n",
    "        return \" \".join(self.accronyms.get(word, word) for word in self.text.split())\n",
    "\n",
    "\n",
    "    def _tokenizing(self):\n",
    "        return word_tokenize(self.text)\n",
    "    \n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def _remove_emalis(self):\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        return re.sub(email_pattern, '', self.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_cls = preprocess(emoticons_dict, stop_words, lemmatizer, acronyms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give an example for see how good this preprocess class is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"Hey @user123, can't wait for the meetup tonight! ðŸ¥³ #excited #networking \n",
    "OMG, did you see the latest episode of #TheTechShow? Mind-blowing theories there... ðŸ¤¯ðŸ’¡ \n",
    "LOL, my code passed all tests on first 12412342133 run - feel like a boss ðŸ˜ŽðŸ‘©â€ðŸ’»\n",
    "Btw, check out this cool article on quantum computing: https://bit.ly/3xyzABC\n",
    "P.S. @competitor did a boo-boo again ðŸ™ˆ #facepalm \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'cant', 'wait', 'for', 'the', 'meetup', 'tonight', '!', 'partyingface', 'excited', 'networking', 'oh', 'my', 'god', 'did', 'you', 'see', 'the', 'latest', 'episode', 'of', 'thetechshow', 'mindblowing', 'theory', 'there', 'explodingheadlightbulb', 'laughing', 'out', 'loud', 'my', 'code', 'passed', 'all', 'test', 'on', 'first', 'run', 'feel', 'like', 'a', 'bos', 'smilingfacewithsunglasseswomantechnologist', 'by', 'the', 'way', 'check', 'out', 'this', 'cool', 'article', 'on', 'quantum', 'computing', 'p', 'did', 'a', 'booboo', 'again', 'seenoevilmonkey', 'facepalm']\n"
     ]
    }
   ],
   "source": [
    "preprocess_cls.assign_text(example)\n",
    "prep_text = preprocess_cls.normalize_tokenize()\n",
    "print(prep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    preprocess_cls.assign_text(text)\n",
    "    return preprocess_cls.normalize_tokenize()\n",
    "\n",
    "sampled_df['tokens'] = sampled_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [oh, no, !, where, did, u, order, from, thats,...\n",
       "1       [a, great, hard, training, weekend, is, over, ...\n",
       "2       [right, off, to, work, only, hour, to, go, unt...\n",
       "3                   [i, am, craving, for, japanese, food]\n",
       "4       [jean, michel, jarre, concert, tomorrow, got, ...\n",
       "                              ...                        \n",
       "9995                             [love, to, see, that, !]\n",
       "9996    [lovely, day, spent, with, the, miss, just, lo...\n",
       "9997    [i, love, the, fact, that, people, are, essent...\n",
       "9998    [time, to, shower, and, tidy, then, going, to,...\n",
       "9999               [and, it, an, excuse, to, get, a, wii]\n",
       "Name: tokens, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import sklearn for spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split equaly  between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_class = sampled_df[sampled_df['target'] == 0]\n",
    "positive_class = sampled_df[sampled_df['target'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train, X_pos_test, y_pos_train, y_pos_test = train_test_split(positive_class['tokens'], positive_class['target'], test_size=0.2, random_state=2)\n",
    "X_neg_train, X_neg_test, y_neg_train, y_neg_test = train_test_split(negative_class['tokens'], negative_class['target'], test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_pos_train, X_neg_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_pos_test, X_neg_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.concat([y_pos_train, y_neg_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.concat([y_pos_test, y_neg_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test[y_test==0].shape, y_test[y_test==4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104608\n",
      "11600\n"
     ]
    }
   ],
   "source": [
    "vocab_set = {}\n",
    "all_tokens = []\n",
    "for index,tokens in X_train.items():\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(len(all_tokens))\n",
    "vocab_set = set(all_tokens)\n",
    "print(len(vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fucntion for Term Frequency(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class freq_matrix:\n",
    "    def __init__(self, X_train, X_test, vocab_set):\n",
    "        self.train = X_train\n",
    "        self.test = X_test\n",
    "        self.vocab_set = vocab_set\n",
    "        self.vocab_set.add('OOV')\n",
    "        print(len(self.vocab_set))\n",
    "        self.unique_words_index = list(self.vocab_set)\n",
    "    \n",
    "    def term_frequency_matrix_train(self):\n",
    "        real_index = 0\n",
    "        tf_matrix = np.zeros(shape=(len(self.train), len(self.vocab_set)), dtype=float)\n",
    "        for _,tokens in self.train.items():\n",
    "            for token in tokens:\n",
    "                word_index = self.unique_words_index.index(token)\n",
    "                tf_matrix[real_index][word_index] += 1\n",
    "            word_index = self.unique_words_index.index('OOV')\n",
    "            tf_matrix[real_index][word_index] = np.mean(tf_matrix[real_index])\n",
    "            real_index += 1\n",
    "        tf_matrix = tf_matrix + 1\n",
    "        return np.where(tf_matrix > 1, 1 + np.log(tf_matrix), 0), tf_matrix\n",
    "    \n",
    "    def term_frequncy_matrix_test(self):\n",
    "        real_index = 0\n",
    "        unseen_data_set = set()\n",
    "        tf_matrix = np.zeros(shape=(len(self.test), len(self.vocab_set)), dtype=float)\n",
    "        for _,tokens in self.test.items():\n",
    "            for token in tokens:\n",
    "                if token in self.vocab_set:\n",
    "                    word_index = self.unique_words_index.index(token)\n",
    "                    tf_matrix[real_index][word_index] += 1\n",
    "                else:\n",
    "                    unseen_data_set.add(token)\n",
    "                    word_index = self.unique_words_index.index('OOV')\n",
    "                    tf_matrix[real_index][word_index] += 1\n",
    "            real_index += 1\n",
    "        tf_matrix = tf_matrix + 1\n",
    "        return np.where(tf_matrix > 1, 1 + np.log(tf_matrix), 0), tf_matrix, unseen_data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11601\n"
     ]
    }
   ],
   "source": [
    "my_freq_matrix = freq_matrix(X_train, X_test, vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11601)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_train_tf, ordinary_tf_Train = my_freq_matrix.term_frequency_matrix_train()\n",
    "log_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11601)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_test_tf, ordinary_tf_test, unseen_data = my_freq_matrix.term_frequncy_matrix_test()\n",
    "log_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786\n",
      "0.15395224549607792\n"
     ]
    }
   ],
   "source": [
    "print(len(unseen_data))\n",
    "print(len(unseen_data)/len(vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_matrix_train(self, log_tf, train_tf):\n",
    "        train_tf_idf = np.zeros(shape=(log_tf.shape))\n",
    "        term_freq = np.copy(train_tf)\n",
    "        df = np.zeros(shape=(train_tf.shape[1]))\n",
    "        idf = np.zeros(shape=(train_tf.shape[1]))\n",
    "        for i,_ in enumerate(self.unique_words_index):\n",
    "                df[i] = (term_freq[:,i] > 1).sum()\n",
    "                idf[i] = np.log(train_tf.shape[0]/(df[i] + 1))\n",
    "        idf[self.unique_words_index.index('OOV')] = np.log(train_tf.shape[0]/1)\n",
    "        for i in range(len(self.unique_words_index)):\n",
    "                train_tf_idf[:,i] = log_tf[:, i] * idf[i]\n",
    "        return train_tf_idf\n",
    "\n",
    "def tf_idf_matrix_test(self, log_tf, test_tf):\n",
    "        train_tf_idf = np.zeros(shape=(log_tf.shape))\n",
    "        term_freq = np.copy(test_tf)\n",
    "        df = np.zeros(shape=(test_tf.shape[1]))\n",
    "        idf = np.zeros(shape=(test_tf.shape[1]))\n",
    "        for i,_ in enumerate(self.unique_words_index):\n",
    "                df[i] = (term_freq[:,i] > 1).sum()\n",
    "                idf[i] = np.log(test_tf.shape[0]/(df[i] + 1))\n",
    "        for i in range(len(self.unique_words_index)):\n",
    "                train_tf_idf[:,i] = log_tf[:, i] * idf[i]\n",
    "        return train_tf_idf\n",
    "\n",
    "freq_matrix.tf_idf_matrix_train = tf_idf_matrix_train\n",
    "freq_matrix.tf_idf_matrix_test = tf_idf_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11601)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_train = my_freq_matrix.tf_idf_matrix_train(log_train_tf, ordinary_tf_Train)\n",
    "tf_idf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11601)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_test = my_freq_matrix.tf_idf_matrix_test(log_test_tf, ordinary_tf_test)\n",
    "tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Methods to calculate PPMI for word-word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "def bigram_freq(self):\n",
    "    all_tokens = []\n",
    "    for _, tokens in self.train.items():\n",
    "        all_tokens.extend(tokens)\n",
    "    return FreqDist(list(ngrams(all_tokens, 2)))\n",
    "\n",
    "def co_occurence_matrix(self):\n",
    "    co_matrix = np.zeros(shape=(len(self.vocab_set), len(self.vocab_set)))\n",
    "    bigrams = self.bigram_freq()\n",
    "    for bigram,value in bigrams.items():\n",
    "        word_index1 = self.unique_words_index.index(bigram[0])\n",
    "        word_index2 = self.unique_words_index.index(bigram[1])\n",
    "        co_matrix[word_index1, word_index2] += value\n",
    "    word_index = self.unique_words_index.index('OOV')\n",
    "    co_matrix[word_index, :] = np.mean(co_matrix, axis=0)\n",
    "    co_matrix[:, word_index] = np.mean(co_matrix, axis=1)\n",
    "    co_matrix += 1\n",
    "    return co_matrix\n",
    "\n",
    "def ppmi_matrix(self, occurence_matrix):\n",
    "    co_matrix = np.copy(occurence_matrix)\n",
    "    total_sums = np.sum(co_matrix)\n",
    "    co_matrix = co_matrix / total_sums\n",
    "    p_rows = np.sum(co_matrix, axis=1) / total_sums\n",
    "    p_cols = np.sum(co_matrix, axis=0) / total_sums\n",
    "    pmi_matrix = np.log2(np.divide(co_matrix, np.outer(p_rows, p_cols)))\n",
    "    ppmi_matrix = np.maximum(pmi_matrix, 0)\n",
    "    return ppmi_matrix\n",
    "\n",
    "\n",
    "\n",
    "freq_matrix.bigram_freq = bigram_freq\n",
    "freq_matrix.co_occurence_matrix = co_occurence_matrix\n",
    "freq_matrix.ppmi_matrix = ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_matrix = my_freq_matrix.co_occurence_matrix()\n",
    "ppmi_matrix = my_freq_matrix.ppmi_matrix(occurence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the documents based on our vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_embeding_train(self, word_ppmi):\n",
    "    doc_word_ppmi = np.zeros(shape=(len(self.train), len(self.vocab_set)), dtype=float)\n",
    "    real_index = 0\n",
    "    for _,tokens in self.train.items():\n",
    "        counter = 0\n",
    "        for token in tokens:\n",
    "            doc_word_ppmi[real_index] += word_ppmi[self.unique_words_index.index(token), :]\n",
    "            counter += 1\n",
    "        if counter != 0:\n",
    "            doc_word_ppmi[real_index] = doc_word_ppmi[real_index] / counter\n",
    "        real_index += 1\n",
    "    return doc_word_ppmi\n",
    "        \n",
    "def doc_embedding_test(self, word_ppmi):\n",
    "    doc_word_ppmi = np.zeros(shape=(len(self.test), len(self.vocab_set)), dtype=float)\n",
    "    real_index = 0\n",
    "    for _,tokens in self.test.items():\n",
    "        counter = 0\n",
    "        for token in tokens:\n",
    "            if token in self.vocab_set:\n",
    "                doc_word_ppmi[real_index] += word_ppmi[self.unique_words_index.index(token), :]\n",
    "            else:\n",
    "                doc_word_ppmi[real_index] += word_ppmi[self.unique_words_index.index('OOV'), :]\n",
    "            counter += 1\n",
    "        if counter!=0:\n",
    "            doc_word_ppmi[real_index] = doc_word_ppmi[real_index] / counter\n",
    "        real_index += 1\n",
    "    return doc_word_ppmi\n",
    "\n",
    "freq_matrix.doc_embeding_train = doc_embeding_train\n",
    "freq_matrix.doc_embedding_test = doc_embedding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11601)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ppmi = my_freq_matrix.doc_embeding_train(ppmi_matrix)\n",
    "train_doc_ppmi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11601)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_ppmi = my_freq_matrix.doc_embedding_test(ppmi_matrix)\n",
    "test_doc_ppmi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(log_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(log_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7305\n",
      "0.686\n",
      "0.7530186608122942\n",
      "0.7179487179487181\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred, pos_label=4))\n",
    "print(precision_score(y_test, y_pred, pos_label=4))\n",
    "print(f1_score(y_test, y_pred, pos_label=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695\n",
      "0.651\n",
      "0.7138157894736842\n",
      "0.6809623430962343\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tf_idf_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred, pos_label=4))\n",
    "print(precision_score(y_test, y_pred, pos_label=4))\n",
    "print(f1_score(y_test, y_pred, pos_label=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(train_doc_ppmi, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_doc_ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591\n",
      "0.478\n",
      "0.6175710594315246\n",
      "0.5388951521984217\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred, pos_label=4))\n",
    "print(precision_score(y_test, y_pred, pos_label=4))\n",
    "print(f1_score(y_test, y_pred, pos_label=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
